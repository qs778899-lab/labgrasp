#!/usr/bin/env python3
import sys
sys.path.append("FoundationPose")
from estimater import *
from datareader import *
from dino_mask import get_mask_from_GD   
from create_camera import CreateRealsense
import cv2
import numpy as np
# import open3d as o3d
import pyrealsense2 as rs
# import torch
import time, os, sys
import json
import threading
from datetime import datetime
import gc
import torch
# from ultralytics.models.sam import Predictor as SAMPredictor
from simple_api import SimpleApi, ForceMonitor, ErrorMonitor
from dobot_gripper import DobotGripper
from transforms3d.euler import euler2mat, mat2euler
from scipy.spatial.transform import Rotation as R
import queue
from spatialmath import SE3, SO3
from grasp_utils import normalize_angle, extract_euler_zyx, print_pose_info
from calculate_grasp_pose_from_object_pose import (
    execute_grasp_from_object_pose, 
    detect_dent_orientation,
    adjust_to_vertical_and_lift,
    descend_with_force_feedback,
    calculate_grasppose_from_objectpose_withoutmove,
    force_guided_spiral_insertion,
)
import rospy
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from ros_utils import ROSSubscriberTest, DummySubscriber



# ---------- æ‰‹çœ¼æ ‡å®š ----------
def load_hand_eye_calibration(json_path="hand_eye_calibration.json"):
    """ä»JSONæ–‡ä»¶åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    calibration = data['T_ee_cam']
    rotation_matrix = np.array(calibration['rotation_matrix'])
    translation_vector = calibration['translation_vector']
    return SE3.Rt(rotation_matrix, translation_vector, check=False)

# ä»ç›¸æœºåæ ‡ç³»åˆ°æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»çš„å˜æ¢çŸ©é˜µ
T_ee_cam = load_hand_eye_calibration()

# ---------- æœºæ¢°è‡‚ ----------
def init_robot():
    dobot = SimpleApi("192.168.5.1", 29999)
    dobot.clear_error()
    dobot.enable_robot()
    dobot.stop()
    # å¯åŠ¨åŠ›ä¼ æ„Ÿå™¨
    dobot.enable_ft_sensor(1)
    time.sleep(1)
    # åŠ›ä¼ æ„Ÿå™¨ç½®é›¶(ä»¥å½“å‰å—åŠ›çŠ¶æ€ä¸ºåŸºå‡†)
    dobot.six_force_home()
    time.sleep(1)
    # åŠ›ç›‘æ§çº¿ç¨‹
    # force_monitor = ForceMonitor(dobot)
    # force_monitor.start_monitoring()
    # error_monitor = ErrorMonitor(dobot)
    # error_monitor.start_monitoring()
    # gripper = DobotGripper(dobot) #notice
    # gripper.connect(init=True)
    return dobot
    # return dobot, gripper #notice


# ---------- ROSèŠ‚ç‚¹ -------------------------------------------------------
# ROSSubscriberTestç±»å·²ç§»è‡³ros_utils.pyæ¨¡å—ä¸­
# ä½¿ç”¨æ–¹æ³•: from ros_utils import ROSSubscriberTest, DummySubscriber


if __name__ == "__main__":
    # åˆå§‹åŒ–ROSèŠ‚ç‚¹ï¼ˆä½¿ç”¨anonymous=Trueé¿å…èŠ‚ç‚¹åå†²çªï¼‰
    rospy.init_node('glassbar_plug_main', anonymous=True)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join("record_images_during_grasp", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    # print(f"å›¾åƒå°†ä¿å­˜åˆ°: {save_dir}")
    
    camera = CreateRealsense("231522072272") 
    # mesh_file = "mesh/cube.obj"
    mesh_file = "mesh/thin_cube.obj"
    debug = 0
    debug_dir = "debug"
    set_logging_format()
    set_seed(0)
    mesh = trimesh.load(mesh_file)
    #? openscadçš„å•ä½æ˜¯mmï¼Œ ä½†æ˜¯è½¬ä¸ºobjæ–‡ä»¶åå•ä½åˆå˜æˆmï¼Œæ‰€ä»¥è¿˜æ˜¯éœ€è¦è½¬æ¢ï¼
    mesh.vertices /= 1000 #! å•ä½è½¬æ¢é™¤ä»¥1000
    # mesh.vertices /= 3
    to_origin, extents = trimesh.bounds.oriented_bounds(mesh)
    bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)
    

    # åˆå§‹åŒ–æœºæ¢°è‡‚
    # dobot, gripper = init_robot() #notice
    dobot = init_robot()
    #notice
    # #? åˆå§‹åŒ–ROSè®¢é˜…è€…ï¼ˆåœ¨åå°daemonçº¿ç¨‹è¿è¡Œï¼Œä¸ä¼šé˜»å¡mainç¨‹åºï¼‰
    # try:
    #     ros_subscriber = ROSSubscriberTest()
    #     ros_thread = threading.Thread(target=ros_subscriber.run, daemon=True)
    #     ros_thread.start()
    #     print("âœ… ROSè®¢é˜…è€…å·²åœ¨åå°å¯åŠ¨ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰")
    #     time.sleep(1)  # çŸ­æš‚ç­‰å¾…ROSèŠ‚ç‚¹å¯åŠ¨
    # except Exception as e:
    #     print(f"âš ï¸  ROSè®¢é˜…è€…å¯åŠ¨å¤±è´¥: {e}")
    #     # ä½¿ç”¨ros_utilsä¸­çš„DummySubscriberä½œä¸ºå ä½å¯¹è±¡ï¼Œé˜²æ­¢åç»­ä»£ç å‡ºé”™
    #     ros_subscriber = DummySubscriber()

    # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œå§¿æ€ä¼˜åŒ–å™¨
    # scorer = ScorePredictor() 
    # refiner = PoseRefinePredictor()
    # glctx = dr.RasterizeCudaContext()
    # # åˆ›å»ºFoundationPoseä¼°è®¡å™¨
    # est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=debug, glctx=glctx)
    # logging.info("estimator initialization done")
    # # è·å–ç›¸æœºå¤–å‚
    # cam_k = np.loadtxt(f'cam_K.txt').reshape(3,3)

    # ------äº¤äº’å¼é€‰æ‹©ç›®æ ‡ç‚¹åæ ‡-------------------------------------------------------
    #? ç›¸æœºå†…å‚æˆ–è€…è°ƒç”¨å‡½æ•°è®¡ç®—è½¬æ¢ä¸å¯¹
    # print("\nå¼€å§‹äº¤äº’å¼åæ ‡é€‰æ‹©...")
    # point_info = camera.get_point_coordinate(window_name="select_target_point")
    
    # pixel_coord = None
    # camera_coord = None
    # depth_value = None
    # target_point = None
    # target_point_in_camera = None
    
    # if point_info is not None:
    #     pixel_coord = point_info['pixel']
    #     camera_coord = point_info['camera_coord']
    #     depth_value = point_info['depth']
        
    #     print(f"\nè·å–åˆ°çš„ç›®æ ‡ç‚¹ä¿¡æ¯:")
    #     # print(f"  åƒç´ åæ ‡: {pixel_coord}")
    #     print(f"  ç›¸æœºåæ ‡ç³» (X, Y, Z): ({camera_coord[0]:.4f}, {camera_coord[1]:.4f}, {camera_coord[2]:.4f}) m") # (X, Y, Z): (0.0582, -0.1203, 0.5940) m
    #     print(f"  æ·±åº¦å€¼: {depth_value:.4f} m\n")
    #     # è½¬æ¢ä¸ºnumpyæ•°ç»„
    #     target_point_in_camera = np.array([camera_coord[0], camera_coord[1], camera_coord[2]])  # ç›¸æœºåæ ‡ç³»ä¸‹çš„3Dåæ ‡
        
    # else:
    #     print("æœªé€‰æ‹©ç›®æ ‡ç‚¹ï¼Œè·³è¿‡è¯¥æ­¥éª¤ç»§ç»­...\n")

    # input("break000")

    #é”™è¯¯: 0.0582, -0.1203, 0.5940
    target_point_in_camera = np.array([0.11175, -0.14139, 0.59187]) # ç›¸æœºåæ ‡ç³»ä¸‹çš„ç‰©ä½“çš„3Dåæ ‡. eg2: x = 0.11175, y = -0.14139, z = 0.59187.  eg3: 

    #-----è®¡ç®—æŠ“å–å§¿æ€ä½†ä¸æŠ“å–-------------------------------------------------------
    #è½¬æ¢ä¸ºæœºå™¨äººåŸºåæ ‡ç³»
    # é…ç½®æŠ“å–å‚æ•°
    # z_xoy_angle = 0 # ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦
    # vertical_euler = [-180, 0, -90]  # å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„rx, ry, rz
    # grasp_tilt_angle = 30  #  ç”±å‚ç›´å‘ä¸‹æŠ“å–æ—‹è½¬ä¸ºæ–œç€å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„æ—‹è½¬è§’åº¦ï¼š åŠ äº†30åº¦ä¼šæœå¤–æ—‹è½¬
    # z_safe_distance= 30  #zæ–¹å‘çš„ä¸€ä¸ªå®‰å…¨è·ç¦»ï¼Œä¹Ÿæ˜¯ä¸ºäº†æŠ“å–ç‰©ä½“é ä¸Šçš„éƒ¨åˆ†ï¼Œå¯çµæ´»è°ƒæ•´
    # T_base_ee_ideal, target_pos_mm, rx, ry, rz = calculate_grasppose_from_objectpose_withoutmove(
    #         dobot=dobot,
    #         gripper=gripper,
    #         T_ee_cam=T_ee_cam,
    #         z_xoy_angle=z_xoy_angle,
    #         vertical_euler=vertical_euler,
    #         grasp_tilt_angle=grasp_tilt_angle,
    #         angle_threshold=10.0,
    #         T_tcp_ee_z= -0.16, 
    #         T_safe_distance= 0.00, 
    #         z_safe_distance=z_safe_distance,
    #         verbose=True,
    #         target_point_camera=target_point_in_camera,)
    # print("target_pos_mm: ", target_pos_mm) #é”™è¯¯eg: [     759.45     -234.16        -391]
    #eg2: 

    
    # ------ç›´æ¥ç§»åŠ¨åˆ°ç»ç’ƒæ£’ä¸Šæ–¹æŠ“å–ï¼Œç›´æ¥ç»™åæ ‡å€¼-------------------------------------------------------
    # dobot.move_to_pose(585, -220, 72, rx, ry, rz, speed=13, acceleration=1) 
    # wait_move = rospy.Rate(1/2)
    # wait_move.sleep()
    # gripper.control(position=13, force=12, speed=27)
    # wait_grasp = rospy.Rate(1/5)
    # wait_grasp.sleep()

    # pose_now = dobot.get_pose()
    # x_adjustment = 42
    # z_adjustment = 60
    # dobot.move_to_pose(pose_now[0]+x_adjustment, pose_now[1], pose_now[2]+z_adjustment, pose_now[3], pose_now[4], pose_now[5], speed=9)
    


    frame_count = 0
    last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
    last_valid_angle = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„ROSè§’åº¦
    last_seen_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„ROSæ—¶é—´æˆ³ï¼ˆtracking_dataï¼‰
    last_seen_img_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„å›¾åƒæ—¶é—´æˆ³
    last_valid_detected_angles = None  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„è§’åº¦åˆ—è¡¨
    last_valid_avg_angle = 0.0  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„å¹³å‡è§’åº¦
        
    # try:
    #     frame_count = 0
    #     last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
    #     last_valid_angle = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„ROSè§’åº¦
    #     last_seen_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„ROSæ—¶é—´æˆ³ï¼ˆtracking_dataï¼‰
    #     last_seen_img_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„å›¾åƒæ—¶é—´æˆ³
    #     last_valid_detected_angles = None  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„è§’åº¦åˆ—è¡¨
    #     last_valid_avg_angle = 0.0  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„å¹³å‡è§’åº¦
        
    #     while True:
    #         # è·å–å½“å‰å¸§
    #         # color = camera.get_frames()['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
    #         # depth = camera.get_frames()['depth']/1000
    #         # ir1 = camera.get_frames()['ir1']
    #         # ir2 = camera.get_frames()['ir2']
    #         frames = camera.get_frames()
    #         if frames is None:
    #             continue
    #         color = frames['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
    #         depth = frames['depth']/1000
    #         ir1 = frames['ir1']
    #         ir2 = frames['ir2']

    #         color_path = os.path.join(save_dir, f"color_frame_{frame_count:06d}.png")
    #         print("befor foundation pose, color_shape: ", color.shape)
    #         cv2.imwrite(color_path, color)
            
            
    #         # æ¯éš”30å¸§è¿›è¡Œä¸€æ¬¡FoundationPoseæ£€æµ‹
    #         if frame_count % 15 == 0:
    #             #ä½¿ç”¨GroundingDINOè¿›è¡Œè¯­ä¹‰ç†è§£æ‰¾åˆ°ç‰©ä½“çš„ç²—ç•¥ä½ç½®ï¼ŒSAMè·å–ç‰©ä½“çš„ç›¸å¯¹ç²¾ç¡®æ©ç 
    #             mask = get_mask_from_GD(color, "red stirring rod")
    #             # mask = get_mask_from_GD(color, "Plastic dropper") 
    #             # mask = get_mask_from_GD(color, "long yellow bar")
    #             # mask = get_mask_from_GD(color, "long red bar")
    #             # print("mask_shape: ", mask.shape)
            
    #             cv2.imshow("mask", mask)
    #             cv2.imshow("color", color)
    #             pose = est.register(K=cam_k, rgb=color, depth=depth, ob_mask=mask, iteration=50)
    #             print(f"ç¬¬{frame_count}å¸§æ£€æµ‹å®Œæˆï¼Œpose: {pose}")
    #             center_pose = pose@np.linalg.inv(to_origin) 
    #             vis = draw_posed_3d_box(cam_k, img=color, ob_in_cam=center_pose, bbox=bbox)
    #             vis = draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=cam_k, thickness=3, transparency=0, is_input_rgb=True)
    #             cv2.imshow('1', vis[...,::-1])
    
    #             mask_path = os.path.join(save_dir, f"mask_frame_{frame_count:06d}.png")
    #             vis_path = os.path.join(save_dir, f"vis_frame_{frame_count:06d}.png")
    #             cv2.imwrite(mask_path, mask)
    #             cv2.imwrite(vis_path, vis[...,::-1])                

    #             # cv2.waitKey(0) #waitKey(0) æ˜¯ä¸€ç§é˜»å¡
    #             # input("break001") #inputä¹Ÿæ˜¯ä¸€ç§é˜»å¡
    #             # print("break001")
    
    #             torch.cuda.empty_cache()
    #             gc.collect()
 
    #             last_valid_pose = center_pose  # ä¿å­˜è¿™æ¬¡æ£€æµ‹çš„ç»“æœ
    #         else:
    #             # ä½¿ç”¨ä¸Šä¸€æ¬¡æ£€æµ‹çš„ç»“æœ
    #             center_pose = last_valid_pose
    #             # print(f"ç¬¬{frame_count}å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœ")

    #         print("center_pose_object: ", center_pose) 
    #         frame_count += 1
    #         if center_pose is not None:
    #             break

    # except KeyboardInterrupt:
    #     print("\n[ç”¨æˆ·ä¸­æ–­] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
    # finally:
    #     cv2.destroyAllWindows()
    #     # dobot.disable_robot()


    key = cv2.waitKey(1)
    # if key == ord('q'):  # æŒ‰qé€€å‡º
    #     break
    # elif key == ord('a'):  # æŒ‰aæ‰§è¡ŒæŠ“å–
    #
    # init_position = 10
    # gripper.control(position=init_position, force=80, speed=10)



#     # å°†center_poseè½¬æ¢ä¸ºnumpyæ•°ç»„
#     center_pose_array = np.array(center_pose, dtype=float)
    
# # -------æ‰§è¡ŒæŠ“å–-------------------------------------------------------
#     # é…ç½®æŠ“å–å‚æ•°
#     z_xoy_angle = 0 # ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦
#     vertical_euler = [-180, 0, -90]  # å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„rx, ry, rz
#     grasp_tilt_angle = 30  #  ç”±å‚ç›´å‘ä¸‹æŠ“å–æ—‹è½¬ä¸ºæ–œç€å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„æ—‹è½¬è§’åº¦ï¼š åŠ äº†30åº¦ä¼šæœå¤–æ—‹è½¬
#     z_safe_distance= 39  #zæ–¹å‘çš„ä¸€ä¸ªå®‰å…¨è·ç¦»ï¼Œä¹Ÿæ˜¯ä¸ºäº†æŠ“å–ç‰©ä½“é ä¸Šçš„éƒ¨åˆ†ï¼Œå¯çµæ´»è°ƒæ•´
    
#     # è°ƒç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–
#     success, T_base_ee_ideal = execute_grasp_from_object_pose(
#         center_pose_array=center_pose_array,
#         dobot=dobot,
#         gripper=gripper,
#         T_ee_cam=T_ee_cam,
#         z_xoy_angle=z_xoy_angle,
#         vertical_euler=vertical_euler,
#         grasp_tilt_angle=grasp_tilt_angle,
#         angle_threshold=10.0,
#         T_tcp_ee_z= -0.16, 
#         T_safe_distance= 0.00, #å¯çµæ´»è°ƒæ•´
#         z_safe_distance=z_safe_distance,
#         gripper_close_pos=15,
#         verbose=True
#     )
    
#     pose_now = dobot.get_pose()
#     x_adjustment = 115
#     z_adjustment = 180
#     dobot.move_to_pose(pose_now[0]+x_adjustment, pose_now[1], pose_now[2]+z_adjustment, pose_now[3], pose_now[4], pose_now[5], speed=7, acceleration=1) 







#     wait1 = rospy.Rate(1.0 / 5.0)
#     wait1.sleep()
# #-------æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘-------------------------------------------------------
#     #mark: å¾ªç¯è·å–ROSåŸå§‹å›¾åƒå¹¶æ£€æµ‹æ–¹å‘ï¼Œç›´åˆ°æ£€æµ‹æˆåŠŸ
#     print("å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘...")

#     detected_angles = None
#     avg_angle = 0.0
#     detection_attempts = 0
    
#     while True:
#         detection_attempts += 1
#         # è·å–ROSåŸå§‹å›¾åƒæ•°æ®
#         raw_image, img_timestamp = ros_subscriber.get_latest_raw_image()
#         has_new_image = raw_image is not None
#         if has_new_image:
#             # æ”¶åˆ°æ–°å›¾åƒï¼Œè¿›è¡Œæ–¹å‘æ£€æµ‹
#             print(f"\nğŸ“· ç¬¬{detection_attempts}æ¬¡å°è¯•: æ£€æµ‹æ–°åŸå§‹å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
#             detected_angles, avg_angle = detect_dent_orientation(raw_image, save_dir=save_dir)
            
#             if detected_angles:
#                 last_valid_detected_angles = detected_angles
#                 last_valid_avg_angle = avg_angle
#                 last_seen_img_ts = img_timestamp
#                 print(f"æˆåŠŸæ£€æµ‹åˆ°ç‰©ä½“æœå‘è§’åº¦: {detected_angles}, å¹³å‡: {avg_angle:.2f}Â°")
#                 print("="*60)
#                 break  
#             else:
#                 print("å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾ï¼Œç»§ç»­ç­‰å¾…...")
#                 time.sleep(0.1)  
#         else:
#             print(f"ç¬¬{detection_attempts}æ¬¡å°è¯•: ç­‰å¾…å›¾åƒæ•°æ®...")
#             time.sleep(0.1)  
        
#         # å¯é€‰ï¼šæœ€å¤§å°è¯•æ¬¡æ•°é™åˆ¶
#         if detection_attempts >= 100:
#             print(" è­¦å‘Š: è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°(100æ¬¡)ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦")
#             detected_angles = []
#             avg_angle = 0.0
#             break



#-----------å¼€å§‹è°ƒæ•´ç»ç’ƒæ£’å§¿æ€-------------------------------------------------------
    # # è°ƒç”¨å°è£…å‡½æ•°ï¼šè°ƒæ•´å§¿æ€è‡³å‚ç›´å¹¶æŠ¬å‡
    # adjust_result = adjust_to_vertical_and_lift(
    #     dobot=dobot,
    #     avg_angle=avg_angle, # æ£€æµ‹åˆ°çš„ç»ç’ƒæ£’å½“å‰å€¾æ–œè§’åº¦ï¼ˆåº¦ï¼‰
    #     grasp_tilt_angle=grasp_tilt_angle,
    #     x_adjustment = 0,
    #     z_adjustment = 0,
    #     verbose=True
    # )

    # wait_rate = rospy.Rate(1.0 / 5.0)  
    # wait_rate.sleep()
    


# --------ç§»åŠ¨åˆ°ç›®æ ‡holeæ­£ä¸Šæ–¹çš„ä½ç½®-------------------------------------------------------

    #ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
    pose_now = dobot.get_pose()
    x_safe_adjustment = -52
    y_safe_adjustment = 30
    z_safe_adjustment = -50
    pose_now = [586, -22, 68, -133.37, 0, -90]
    #target_pos_mm
    dobot.move_to_pose(pose_now[0], pose_now[1], pose_now[2], pose_now[3], pose_now[4], pose_now[5], speed=9)

    wait_nearby = rospy.Rate(1.0 / 3.5)  
    wait_nearby.sleep()


#-------é€šè¿‡åŠ›æ§ä¸‹é™å¹¶åœ¨XYå¹³é¢æ‰§è¡Œèºæ—‹å¾®è°ƒï¼Œå°è¯•æ’å…¥hole-------------------------------------------------------
    insertion_result = force_guided_spiral_insertion(
        dobot=dobot,
        verbose=True,
    )

    print("èºæ—‹æ’å…¥ç»“æœ:", insertion_result)

    # # è°ƒç”¨å°è£…å‡½æ•°ï¼šå‚ç›´ä¸‹é™å¹¶æ£€æµ‹åŠ›åé¦ˆ
    # descend_result = descend_with_force_feedback(
    #     dobot=dobot,
    #     move_step=1,
    #     max_steps=700,
    #     force_threshold=1.5,
    #     verbose=True
    # )


#if __name__ == "__main__":
#!  main() å°è£…è¿›å»