#!/usr/bin/env python3
import sys
sys.path.append("FoundationPose")
from estimater import *
from datareader import *
from dino_mask import get_mask_from_GD   
from create_camera import CreateRealsense
import cv2
import numpy as np
# import open3d as o3d
import pyrealsense2 as rs
# import torch
import time, os, sys
import json
import threading
from datetime import datetime
import gc
import torch
# from ultralytics.models.sam import Predictor as SAMPredictor
from simple_api import SimpleApi, ForceMonitor, ErrorMonitor
from dobot_gripper import DobotGripper
from transforms3d.euler import euler2mat, mat2euler
from scipy.spatial.transform import Rotation as R
import queue
from spatialmath import SE3, SO3
from grasp_utils import normalize_angle, extract_euler_zyx, print_pose_info
from calculate_grasp_pose_from_object_pose import (
    execute_grasp_from_object_pose, 
    detect_dent_orientation,
    adjust_to_vertical_and_lift,
    descend_with_force_feedback
)
import rospy
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from ros_utils import ROSSubscriberTest, DummySubscriber



# ---------- æ‰‹çœ¼æ ‡å®š ----------
def load_hand_eye_calibration(json_path="hand_eye_calibration.json"):
    """ä»JSONæ–‡ä»¶åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    calibration = data['T_ee_cam']
    rotation_matrix = np.array(calibration['rotation_matrix'])
    translation_vector = calibration['translation_vector']
    return SE3.Rt(rotation_matrix, translation_vector, check=False)

# ä»ç›¸æœºåæ ‡ç³»åˆ°æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»çš„å˜æ¢çŸ©é˜µ
T_ee_cam = load_hand_eye_calibration()

# ---------- æœºæ¢°è‡‚ ----------
def init_robot():
    dobot = SimpleApi("192.168.5.1", 29999)
    dobot.clear_error()
    dobot.enable_robot()
    dobot.stop()
    # å¯åŠ¨åŠ›ä¼ æ„Ÿå™¨
    dobot.enable_ft_sensor(1)
    time.sleep(1)
    # åŠ›ä¼ æ„Ÿå™¨ç½®é›¶(ä»¥å½“å‰å—åŠ›çŠ¶æ€ä¸ºåŸºå‡†)
    dobot.six_force_home()
    time.sleep(1)
    # åŠ›ç›‘æ§çº¿ç¨‹
    # force_monitor = ForceMonitor(dobot)
    # force_monitor.start_monitoring()
    # error_monitor = ErrorMonitor(dobot)
    # error_monitor.start_monitoring()
    gripper = DobotGripper(dobot)
    gripper.connect(init=True)
    return dobot, gripper


# ---------- ROSèŠ‚ç‚¹ -------------------------------------------------------
# ROSSubscriberTestç±»å·²ç§»è‡³ros_utils.pyæ¨¡å—ä¸­
# ä½¿ç”¨æ–¹æ³•: from ros_utils import ROSSubscriberTest, DummySubscriber


if __name__ == "__main__":
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join("record_images_during_grasp", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    # print(f"å›¾åƒå°†ä¿å­˜åˆ°: {save_dir}")
    
    # åˆ›å»ºè§’åº¦æ•°æ®è®°å½•æ–‡ä»¶
    angle_log_path = os.path.join(save_dir, "angle_log.csv")
    with open(angle_log_path, 'w') as f:
        f.write("frame,timestamp,angle_z_deg,detected_angles,avg_angle\n")
    # print(f"è§’åº¦æ•°æ®å°†ä¿å­˜åˆ°: {angle_log_path}")
    
    camera = CreateRealsense("231522072272") 
    # mesh_file = "mesh/cube.obj"
    mesh_file = "mesh/thin_cube.obj"
    debug = 0
    debug_dir = "debug"
    set_logging_format()
    set_seed(0)
    mesh = trimesh.load(mesh_file)
    #? openscadçš„å•ä½æ˜¯mmï¼Œ ä½†æ˜¯è½¬ä¸ºobjæ–‡ä»¶åå•ä½åˆå˜æˆmï¼Œæ‰€ä»¥è¿˜æ˜¯éœ€è¦è½¬æ¢ï¼
    mesh.vertices /= 1000 #! å•ä½è½¬æ¢é™¤ä»¥1000
    # mesh.vertices /= 3
    to_origin, extents = trimesh.bounds.oriented_bounds(mesh)
    bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)

    # åˆå§‹åŒ–æœºæ¢°è‡‚
    dobot, gripper = init_robot()

    #? åˆå§‹åŒ–ROSè®¢é˜…è€…ï¼ˆåœ¨åå°daemonçº¿ç¨‹è¿è¡Œï¼Œä¸ä¼šé˜»å¡mainç¨‹åºï¼‰
    try:
        ros_subscriber = ROSSubscriberTest()
        ros_thread = threading.Thread(target=ros_subscriber.run, daemon=True)
        ros_thread.start()
        print("âœ… ROSè®¢é˜…è€…å·²åœ¨åå°å¯åŠ¨ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰")
        time.sleep(1)  # çŸ­æš‚ç­‰å¾…ROSèŠ‚ç‚¹å¯åŠ¨
    except Exception as e:
        print(f"âš ï¸  ROSè®¢é˜…è€…å¯åŠ¨å¤±è´¥: {e}")
        # ä½¿ç”¨ros_utilsä¸­çš„DummySubscriberä½œä¸ºå ä½å¯¹è±¡ï¼Œé˜²æ­¢åç»­ä»£ç å‡ºé”™
        ros_subscriber = DummySubscriber()

    # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œå§¿æ€ä¼˜åŒ–å™¨
    scorer = ScorePredictor() 
    refiner = PoseRefinePredictor()
    glctx = dr.RasterizeCudaContext()
    # åˆ›å»ºFoundationPoseä¼°è®¡å™¨
    est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=debug, glctx=glctx)
    logging.info("estimator initialization done")
    # è·å–ç›¸æœºå†…å‚
    cam_k = np.loadtxt(f'cam_K.txt').reshape(3,3)

    
    try:
        frame_count = 0
        last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
        last_valid_angle = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„ROSè§’åº¦
        last_seen_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„ROSæ—¶é—´æˆ³ï¼ˆtracking_dataï¼‰
        last_seen_img_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„å›¾åƒæ—¶é—´æˆ³
        last_valid_detected_angles = None  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„è§’åº¦åˆ—è¡¨
        last_valid_avg_angle = 0.0  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„å¹³å‡è§’åº¦
        
        while True:
            # è·å–å½“å‰å¸§
            # color = camera.get_frames()['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            # depth = camera.get_frames()['depth']/1000
            # ir1 = camera.get_frames()['ir1']
            # ir2 = camera.get_frames()['ir2']
            frames = camera.get_frames()
            if frames is None:
                continue
            color = frames['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            depth = frames['depth']/1000
            ir1 = frames['ir1']
            ir2 = frames['ir2']

            color_path = os.path.join(save_dir, f"color_frame_{frame_count:06d}.png")
            print("befor foundation pose, color_shape: ", color.shape)
            cv2.imwrite(color_path, color)
            
            
            # æ¯éš”30å¸§è¿›è¡Œä¸€æ¬¡FoundationPoseæ£€æµ‹
            if frame_count % 15 == 0:
                #ä½¿ç”¨GroundingDINOè¿›è¡Œè¯­ä¹‰ç†è§£æ‰¾åˆ°ç‰©ä½“çš„ç²—ç•¥ä½ç½®ï¼ŒSAMè·å–ç‰©ä½“çš„ç›¸å¯¹ç²¾ç¡®æ©ç 
                mask = get_mask_from_GD(color, "red stirring rod")
                # mask = get_mask_from_GD(color, "Plastic dropper") 
                # mask = get_mask_from_GD(color, "long yellow bar")
                # mask = get_mask_from_GD(color, "long red bar")
                # print("mask_shape: ", mask.shape)
            
                cv2.imshow("mask", mask)
                cv2.imshow("color", color)
                pose = est.register(K=cam_k, rgb=color, depth=depth, ob_mask=mask, iteration=50)
                print(f"ç¬¬{frame_count}å¸§æ£€æµ‹å®Œæˆï¼Œpose: {pose}")
                center_pose = pose@np.linalg.inv(to_origin) #! è¿™ä¸ªæ‰æ˜¯ç‰©ä½“ä¸­å¿ƒç‚¹çš„Pose
                vis = draw_posed_3d_box(cam_k, img=color, ob_in_cam=center_pose, bbox=bbox)
                vis = draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=cam_k, thickness=3, transparency=0, is_input_rgb=True)
                cv2.imshow('1', vis[...,::-1])
    
                mask_path = os.path.join(save_dir, f"mask_frame_{frame_count:06d}.png")
                vis_path = os.path.join(save_dir, f"vis_frame_{frame_count:06d}.png")
                cv2.imwrite(mask_path, mask)
                cv2.imwrite(vis_path, vis[...,::-1])                

                # cv2.waitKey(0) #waitKey(0) æ˜¯ä¸€ç§é˜»å¡
                # input("break001") #inputä¹Ÿæ˜¯ä¸€ç§é˜»å¡
                # print("break001")
    
                torch.cuda.empty_cache()
                gc.collect()
 
                last_valid_pose = center_pose  # ä¿å­˜è¿™æ¬¡æ£€æµ‹çš„ç»“æœ
            else:
                # ä½¿ç”¨ä¸Šä¸€æ¬¡æ£€æµ‹çš„ç»“æœ
                center_pose = last_valid_pose
                # print(f"ç¬¬{frame_count}å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœ")

            print("center_pose_object: ", center_pose) 
            frame_count += 1
            if center_pose is not None:
                break

    except KeyboardInterrupt:
        print("\n[ç”¨æˆ·ä¸­æ–­] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
    finally:
        cv2.destroyAllWindows()
        # dobot.disable_robot()


    key = cv2.waitKey(1)
    # if key == ord('q'):  # æŒ‰qé€€å‡º
    #     break
    # elif key == ord('a'):  # æŒ‰aæ‰§è¡ŒæŠ“å–
    #
    # init_position = 10
    # gripper.control(position=init_position, force=80, speed=10)


    #mark: è·å–ROSè·Ÿè¸ªæ•°æ®ï¼ˆéé˜»å¡ï¼‰
    tracking_data = ros_subscriber.get_latest_tracking_data()
    has_new_msg = tracking_data['valid'] and (
        last_seen_ts is None or tracking_data['timestamp'] > last_seen_ts
    )
    if has_new_msg:
        # æ”¶åˆ°æ–°ROSæ•°æ®ï¼Œæ›´æ–°å¹¶ä½¿ç”¨æœ€æ–°è§’åº¦
        angle_z_deg = tracking_data['angle_z_deg']
        last_valid_angle = angle_z_deg
        last_seen_ts = tracking_data['timestamp']
        print(f"ğŸ”„ ä½¿ç”¨ROSè·Ÿè¸ªè§’åº¦: {angle_z_deg:.2f}Â° ")
    else:
        # æ²¡æœ‰æ–°ROSæ•°æ®
        if last_valid_angle is not None:
            angle_z_deg = last_valid_angle
            print(f"ä½¿ç”¨ä¸Šæ¬¡ROSè§’åº¦: {angle_z_deg:.2f}Â° (å½“å‰æ— æ–°æ•°æ®)")
        else:
            angle_z_deg = -45  # æœé‡Œ
            print("ä»æœªæ¥æ”¶åˆ°ROSæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦: -45Â°")

    # å°†center_poseè½¬æ¢ä¸ºnumpyæ•°ç»„
    center_pose_array = np.array(center_pose, dtype=float)
    
# -------æ‰§è¡ŒæŠ“å–-------------------------------------------------------
    # é…ç½®æŠ“å–å‚æ•°
    z_xoy_angle = 0 # ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦
    vertical_euler = [-180, 0, -90]  # å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„rx, ry, rz
    grasp_tilt_angle = 30  #  ç”±å‚ç›´å‘ä¸‹æŠ“å–æ—‹è½¬ä¸ºæ–œç€å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„æ—‹è½¬è§’åº¦ï¼š åŠ äº†30åº¦ä¼šæœå¤–æ—‹è½¬
    z_safe_distance= 39  #zæ–¹å‘çš„ä¸€ä¸ªå®‰å…¨è·ç¦»ï¼Œä¹Ÿæ˜¯ä¸ºäº†æŠ“å–ç‰©ä½“é ä¸Šçš„éƒ¨åˆ†ï¼Œå¯çµæ´»è°ƒæ•´
    
    # è°ƒç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–
    success, T_base_ee_ideal = execute_grasp_from_object_pose(
        center_pose_array=center_pose_array,
        dobot=dobot,
        gripper=gripper,
        T_ee_cam=T_ee_cam,
        z_xoy_angle=z_xoy_angle,
        vertical_euler=vertical_euler,
        grasp_tilt_angle=grasp_tilt_angle,
        angle_threshold=10.0,
        T_tcp_ee_z= -0.16, 
        T_safe_distance= 0.00, #å¯çµæ´»è°ƒæ•´
        z_safe_distance=z_safe_distance,
        gripper_close_pos=15,
        verbose=True
    )
    
    pose_now = dobot.get_pose()
    x_adjustment = 115
    z_adjustment = 180
    dobot.move_to_pose(pose_now[0]+x_adjustment, pose_now[1], pose_now[2]+z_adjustment, pose_now[3], pose_now[4], pose_now[5], speed=7, acceleration=1) 


#-------æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘-------------------------------------------------------
    #mark: å¾ªç¯è·å–ROSåŸå§‹å›¾åƒå¹¶æ£€æµ‹æ–¹å‘ï¼Œç›´åˆ°æ£€æµ‹æˆåŠŸ
    print("å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘...")

    detected_angles = None
    avg_angle = 0.0
    detection_attempts = 0
    
    while True:
        detection_attempts += 1
        # è·å–ROSåŸå§‹å›¾åƒæ•°æ®
        raw_image, img_timestamp = ros_subscriber.get_latest_raw_image()
        has_new_image = raw_image is not None
        if has_new_image:
            # æ”¶åˆ°æ–°å›¾åƒï¼Œè¿›è¡Œæ–¹å‘æ£€æµ‹
            print(f"\nğŸ“· ç¬¬{detection_attempts}æ¬¡å°è¯•: æ£€æµ‹æ–°åŸå§‹å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
            detected_angles, avg_angle = detect_dent_orientation(raw_image, save_dir=save_dir)
            
            if detected_angles:
                last_valid_detected_angles = detected_angles
                last_valid_avg_angle = avg_angle
                last_seen_img_ts = img_timestamp
                print(f"æˆåŠŸæ£€æµ‹åˆ°ç‰©ä½“æœå‘è§’åº¦: {detected_angles}, å¹³å‡: {avg_angle:.2f}Â°")
                print("="*60)
                break  
            else:
                print("å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾ï¼Œç»§ç»­ç­‰å¾…...")
                time.sleep(0.1)  
        else:
            print(f"ç¬¬{detection_attempts}æ¬¡å°è¯•: ç­‰å¾…å›¾åƒæ•°æ®...")
            time.sleep(0.1)  
        
        # å¯é€‰ï¼šæœ€å¤§å°è¯•æ¬¡æ•°é™åˆ¶
        if detection_attempts >= 100:
            print(" è­¦å‘Š: è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°(100æ¬¡)ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦")
            detected_angles = []
            avg_angle = 0.0
            break
    # è®°å½•angle_z_deg å’Œ detected_anglesåˆ°logæ–‡ä»¶
    with open(angle_log_path, 'a') as f:
        angles_str = str(detected_angles) if detected_angles is not None else "None"
        f.write(f"{frame_count},{time.time():.3f},{angle_z_deg:.2f},{angles_str},{avg_angle:.2f}\n")



#-----------å¼€å§‹è°ƒæ•´ç»ç’ƒæ£’å§¿æ€-------------------------------------------------------
    # è°ƒç”¨å°è£…å‡½æ•°ï¼šè°ƒæ•´å§¿æ€è‡³å‚ç›´å¹¶æŠ¬å‡
    adjust_result = adjust_to_vertical_and_lift(
        dobot=dobot,
        avg_angle=avg_angle, # æ£€æµ‹åˆ°çš„ç»ç’ƒæ£’å½“å‰å€¾æ–œè§’åº¦ï¼ˆåº¦ï¼‰
        grasp_tilt_angle=grasp_tilt_angle,
        verbose=True
    )

    wait_rate = rospy.Rate(1.0 / 10.0)  
    wait_rate.sleep()
    
    # è°ƒç”¨å°è£…å‡½æ•°ï¼šå‚ç›´ä¸‹é™å¹¶æ£€æµ‹åŠ›åé¦ˆ
    descend_result = descend_with_force_feedback(
        dobot=dobot,
        move_step=1,
        max_steps=700,
        force_threshold=1.5,
        verbose=True
    )

    #ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
    pose_now = dobot.get_pose()
    x_target, y_target, z_target= 450, -150, 12
    rx_target, ry_target, rz_target= pose_now[3], pose_now[4], pose_now[5]
    # dobot.move_to_pose(x_target, y_target, z_target, rx_target, ry_target, rz_target, speed=9)


    # å¯é€‰ï¼šè¿”å›homeä½ç½®ï¼ˆæ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šï¼‰
    # dobot.move_to_pose(435.4503, 281.809, 348.9125, -179.789, -0.8424, 14.4524, speed=9)

