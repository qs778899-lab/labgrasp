#!/usr/bin/env python3
import sys
sys.path.append("FoundationPose")
from estimater import *
from datareader import *
from dino_mask import get_mask_from_GD   
from create_camera import CreateRealsense
import cv2
import numpy as np
# import open3d as o3d
import pyrealsense2 as rs
# import torch
import time, os, sys
import json
import threading
from datetime import datetime
import gc
import torch
# from ultralytics.models.sam import Predictor as SAMPredictor
from simple_api import SimpleApi, ForceMonitor, ErrorMonitor
from dobot_gripper import DobotGripper
from transforms3d.euler import euler2mat, mat2euler
from scipy.spatial.transform import Rotation as R
import queue
from spatialmath import SE3, SO3
from grasp_utils import normalize_angle, extract_euler_zyx, print_pose_info
from calculate_grasp_pose_from_object_pose import execute_grasp_from_object_pose, detect_dent_orientation
import rospy
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import Image
from cv_bridge import CvBridge



# ---------- æ‰‹çœ¼æ ‡å®š ----------
def load_hand_eye_calibration(json_path="hand_eye_calibration.json"):
    """ä»JSONæ–‡ä»¶åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    calibration = data['T_ee_cam']
    rotation_matrix = np.array(calibration['rotation_matrix'])
    translation_vector = calibration['translation_vector']
    return SE3.Rt(rotation_matrix, translation_vector, check=False)

# ä»ç›¸æœºåæ ‡ç³»åˆ°æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»çš„å˜æ¢çŸ©é˜µ
T_ee_cam = load_hand_eye_calibration()

# ---------- æœºæ¢°è‡‚ ----------
def init_robot():
    dobot = SimpleApi("192.168.5.1", 29999)
    dobot.clear_error()
    dobot.enable_robot()
    dobot.stop()
    # å¯åŠ¨åŠ›ä¼ æ„Ÿå™¨
    dobot.enable_ft_sensor(1)
    time.sleep(1)
    # åŠ›ä¼ æ„Ÿå™¨ç½®é›¶(ä»¥å½“å‰å—åŠ›çŠ¶æ€ä¸ºåŸºå‡†)
    dobot.six_force_home()
    time.sleep(1)
    # åŠ›ç›‘æ§çº¿ç¨‹
    # force_monitor = ForceMonitor(dobot)
    # force_monitor.start_monitoring()
    # error_monitor = ErrorMonitor(dobot)
    # error_monitor.start_monitoring()
    gripper = DobotGripper(dobot)
    gripper.connect(init=True)
    return dobot, gripper


# ---------- ROSèŠ‚ç‚¹ ----------
class ROSSubscriberTest:
    def __init__(self):
        """åˆå§‹åŒ–ROSèŠ‚ç‚¹å’Œè®¢é˜…è€…"""
        rospy.init_node('ros_subscriber_test', anonymous=True)
        
        # åˆå§‹åŒ–cv_bridgeç”¨äºå›¾åƒè½¬æ¢
        self.bridge = CvBridge()

        #mark: æ•´ä½“çš„rosæ¥æ”¶ä¿¡æ¯æ–¹æ¡ˆï¼š callback_function + ç¼“å­˜(atest_tracking_data,latest_image) + è®¿é—®å‡½æ•°(get_latest_tracking_data,get_latest_image)
        
        # ç¼“å­˜æœ€æ–°çš„tracking_data
        self.latest_tracking_data = {
            'angle_z_deg': 0.0,
            'b': 0.0,
            'x': 0.0,
            'y': 0.0,
            'timestamp': 0.0,
            'valid': False
        }
        self.data_lock = threading.Lock()
        
        # ç¼“å­˜æœ€æ–°çš„image
        self.latest_image = None
        self.image_timestamp = 0.0
        self.image_lock = threading.Lock()

        
        # è®¢é˜…tracking_data topic
        self.tracking_sub = rospy.Subscriber(
            'tracking_data', #topic name: tracking_data
            Float64MultiArray, 
            self.tracking_callback #mark: callback_function 
        )
        
        # è®¢é˜…object_orientation topic  
        self.image_sub = rospy.Subscriber(
            'image_object_orientation', #topic name: image_object_orientation
            Image,
            self.image_callback
        )
        
        print("ROSè®¢é˜…è€…å·²å¯åŠ¨ï¼Œç­‰å¾…æ•°æ®...")
        
    def tracking_callback(self, msg):
        """å¤„ç†tracking_dataæ¶ˆæ¯"""
        if len(msg.data) >= 4:
            angle_z_deg = msg.data[0]
            b = msg.data[1] 
            x = msg.data[2]
            y = msg.data[3]

            with self.data_lock:
                self.latest_tracking_data.update({
                    'angle_z_deg': angle_z_deg,
                    'b': b,
                    'x': x,
                    'y': y,
                    'timestamp': time.time(),
                    'valid': True
                })
            
            # print(f"ğŸ“Š è·Ÿè¸ªæ•°æ®: è§’åº¦={angle_z_deg:.2f}Â°, æˆªè·={b:.6f}, ä½ç½®=({x:.6f}, {y:.6f})")
        else:
            pass
            # print(f"âš ï¸  è·Ÿè¸ªæ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›4ä¸ªå€¼ï¼Œå®é™…æ”¶åˆ°{len(msg.data)}ä¸ªå€¼")
    
    def image_callback(self, msg):
        """å¤„ç†object_orientationå›¾åƒæ¶ˆæ¯"""
        try:
            # print(f"[DEBUG] å›¾åƒå›è°ƒè¢«è§¦å‘ï¼æ¶ˆæ¯ç±»å‹: {type(msg)}")
            # print(f"[DEBUG] å›¾åƒç¼–ç : {msg.encoding}, å°ºå¯¸: {msg.width}x{msg.height}")
            
            # å°†ROSå›¾åƒæ¶ˆæ¯è½¬æ¢ä¸ºOpenCVæ ¼å¼
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # çº¿ç¨‹å®‰å…¨åœ°ä¿å­˜å›¾åƒ
            with self.image_lock:
                self.latest_image = cv_image.copy()
                self.image_timestamp = time.time()
            
            # # æ˜¾ç¤ºå›¾åƒ
            # cv2.imshow("Object Orientation", cv_image)
            # cv2.waitKey(1)  # éé˜»å¡ç­‰å¾…ï¼Œå…è®¸å…¶ä»–å¤„ç†
            height, width = cv_image.shape[:2]
            # print(f"ğŸ–¼ï¸  æˆåŠŸæ¥æ”¶å¹¶ä¿å­˜å›¾åƒ: {width}x{height} pixels")
            
        except Exception as e:
            print(f"âŒ å›¾åƒå¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    #mark: åœ¨callback_functionåŸºç¡€ä¸Šï¼Œè®¿é—®ç¼“å­˜çš„æœ€æ–°æ•°æ®
    def get_latest_tracking_data(self):
        """è·å–æœ€æ–°çš„è·Ÿè¸ªæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.data_lock:
            return self.latest_tracking_data.copy()
    
    def get_latest_image(self):
        """è·å–æœ€æ–°çš„å›¾åƒæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.image_lock:
            if self.latest_image is not None:
                return self.latest_image.copy(), self.image_timestamp
            else:
                return None, 0.0
    
    def run(self):
        """è¿è¡Œè®¢é˜…è€…"""
        try:
            # éé˜»å¡ä¿æ´»å¾ªç¯ï¼šç­‰å¾…ROSäº‹ä»¶ï¼Œä½†ä¸ä¸»åŠ¨é€€å‡º
            while not rospy.is_shutdown():
                time.sleep(0.05)
        except KeyboardInterrupt:
            print("\n rosä¸­æ–­,æ­£åœ¨é€€å‡º...")


if __name__ == "__main__":
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join("record_images_during_grasp", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    # print(f"å›¾åƒå°†ä¿å­˜åˆ°: {save_dir}")
    
    # åˆ›å»ºè§’åº¦æ•°æ®è®°å½•æ–‡ä»¶
    angle_log_path = os.path.join(save_dir, "angle_log.csv")
    with open(angle_log_path, 'w') as f:
        f.write("frame,timestamp,angle_z_deg,detected_angles\n")
    # print(f"è§’åº¦æ•°æ®å°†ä¿å­˜åˆ°: {angle_log_path}")
    
    camera = CreateRealsense("231522072272") 
    # mesh_file = "mesh/cube.obj"
    mesh_file = "mesh/thin_cube.obj"
    debug = 0
    debug_dir = "debug"
    set_logging_format()
    set_seed(0)
    mesh = trimesh.load(mesh_file)
    #? openscadçš„å•ä½æ˜¯mmï¼Œ ä½†æ˜¯è½¬ä¸ºobjæ–‡ä»¶åå•ä½åˆå˜æˆmï¼Œæ‰€ä»¥è¿˜æ˜¯éœ€è¦è½¬æ¢ï¼
    mesh.vertices /= 1000 #! å•ä½è½¬æ¢é™¤ä»¥1000
    # mesh.vertices /= 3
    to_origin, extents = trimesh.bounds.oriented_bounds(mesh)
    bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)

    # åˆå§‹åŒ–æœºæ¢°è‡‚
    dobot, gripper = init_robot()

    #? åˆå§‹åŒ–ROSè®¢é˜…è€…ï¼ˆåœ¨åå°daemonçº¿ç¨‹è¿è¡Œï¼Œä¸ä¼šé˜»å¡mainç¨‹åºï¼‰
    try:
        ros_subscriber = ROSSubscriberTest()
        ros_thread = threading.Thread(target=ros_subscriber.run, daemon=True)
        ros_thread.start()
        print("âœ… ROSè®¢é˜…è€…å·²åœ¨åå°å¯åŠ¨ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰")
        time.sleep(1)  # çŸ­æš‚ç­‰å¾…ROSèŠ‚ç‚¹å¯åŠ¨
    except Exception as e:
        print(f"âš ï¸  ROSè®¢é˜…è€…å¯åŠ¨å¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½å¯¹è±¡ï¼Œé˜²æ­¢åç»­ä»£ç å‡ºé”™
        class DummySubscriber:
            def get_latest_tracking_data(self):
                return {'valid': False, 'angle_z_deg': 0.0, 'b': 0.0, 'x': 0.0, 'y': 0.0, 'timestamp': 0.0}
        ros_subscriber = DummySubscriber()

    # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œå§¿æ€ä¼˜åŒ–å™¨
    scorer = ScorePredictor() 
    refiner = PoseRefinePredictor()
    glctx = dr.RasterizeCudaContext()
    # åˆ›å»ºFoundationPoseä¼°è®¡å™¨
    est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=debug, glctx=glctx)
    logging.info("estimator initialization done")
    # è·å–ç›¸æœºå†…å‚
    cam_k = np.loadtxt(f'cam_K.txt').reshape(3,3)

    
    try:
        frame_count = 0
        last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
        last_valid_angle = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„ROSè§’åº¦
        last_seen_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„ROSæ—¶é—´æˆ³ï¼ˆtracking_dataï¼‰
        last_seen_img_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„å›¾åƒæ—¶é—´æˆ³
        last_valid_detected_angles = None  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„è§’åº¦
        
        while True:
            # è·å–å½“å‰å¸§
            # color = camera.get_frames()['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            # depth = camera.get_frames()['depth']/1000
            # ir1 = camera.get_frames()['ir1']
            # ir2 = camera.get_frames()['ir2']
            # è·å–å½“å‰å¸§ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼Œå¤ç”¨è¿”å›çš„æ‰€æœ‰é€šé“ï¼‰
            frames = camera.get_frames()
            if frames is None:
                continue
            color = frames['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            depth = frames['depth']/1000
            ir1 = frames['ir1']
            ir2 = frames['ir2']

            color_path = os.path.join(save_dir, f"color_frame_{frame_count:06d}.png")
            print("befor foundation pose, color_shape: ", color.shape)
            
            # cv2.imwrite("ir1.png", ir1)
            # cv2.imwrite("ir2.png", ir2)
            cv2.imwrite(color_path, color)
            
            
            # æ¯éš”70å¸§è¿›è¡Œä¸€æ¬¡FoundationPoseæ£€æµ‹
            if frame_count % 70 == 0:
                #ä½¿ç”¨GroundingDINOè¿›è¡Œè¯­ä¹‰ç†è§£æ‰¾åˆ°ç‰©ä½“çš„ç²—ç•¥ä½ç½®ï¼ŒSAMè·å–ç‰©ä½“çš„ç›¸å¯¹ç²¾ç¡®æ©ç 
                mask = get_mask_from_GD(color, "red stirring rod")
                # mask = get_mask_from_GD(color, "Plastic dropper") 
                # mask = get_mask_from_GD(color, "long yellow bar")
                # mask = get_mask_from_GD(color, "long red bar")
                # print("mask_shape: ", mask.shape)
            
                cv2.imshow("mask", mask)
                cv2.imshow("color", color)
                pose = est.register(K=cam_k, rgb=color, depth=depth, ob_mask=mask, iteration=50)
                print(f"ç¬¬{frame_count}å¸§æ£€æµ‹å®Œæˆï¼Œpose: {pose}")
                center_pose = pose@np.linalg.inv(to_origin) #! è¿™ä¸ªæ‰æ˜¯ç‰©ä½“ä¸­å¿ƒç‚¹çš„Pose
                vis = draw_posed_3d_box(cam_k, img=color, ob_in_cam=center_pose, bbox=bbox)
                vis = draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=cam_k, thickness=3, transparency=0, is_input_rgb=True)
                
                # ä¿å­˜å›¾åƒåˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹
                
                mask_path = os.path.join(save_dir, f"mask_frame_{frame_count:06d}.png")
                vis_path = os.path.join(save_dir, f"vis_frame_{frame_count:06d}.png")
                
                
                cv2.imwrite(mask_path, mask)
                cv2.imwrite(vis_path, vis[...,::-1])
                # print(f"å·²ä¿å­˜å›¾åƒ: {color_path}, {mask_path}, {vis_path}")
                
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    gpu_mem_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                    gpu_mem_reserved = torch.cuda.memory_reserved() / 1024**3  # GB
                    print(f"GPUå†…å­˜: å·²åˆ†é…={gpu_mem_allocated:.2f}GB, å·²ä¿ç•™={gpu_mem_reserved:.2f}GB")
                
                cv2.imshow('1', vis[...,::-1])
 
 
                # cv2.waitKey(0) #waitKey(0) æ˜¯ä¸€ç§é˜»å¡
                # input("break001") #inputä¹Ÿæ˜¯ä¸€ç§é˜»å¡
                # print("break001")
                
                #? æ¸…ç†å†…å­˜ (è¿™ä¸ªæœ‰ç”¨å—ï¼Ÿ)
                torch.cuda.empty_cache()
                gc.collect()
 
                last_valid_pose = center_pose  # ä¿å­˜è¿™æ¬¡æ£€æµ‹çš„ç»“æœ
            else:
                # ä½¿ç”¨ä¸Šä¸€æ¬¡æ£€æµ‹çš„ç»“æœ
                center_pose = last_valid_pose
                # print(f"ç¬¬{frame_count}å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœ")
            

            print("center_pose_object: ", center_pose) 
            frame_count += 1

            key = cv2.waitKey(1)
            # if key == ord('q'):  # æŒ‰qé€€å‡º
            #     break
            # elif key == ord('a'):  # æŒ‰aæ‰§è¡ŒæŠ“å–

            #mark: è·å–ROSè·Ÿè¸ªæ•°æ®ï¼ˆéé˜»å¡ï¼Œä¸ä¼šåœæ­¢mainç¨‹åºï¼‰
            tracking_data = ros_subscriber.get_latest_tracking_data()
            has_new_msg = tracking_data['valid'] and (
                last_seen_ts is None or tracking_data['timestamp'] > last_seen_ts
            )
            if has_new_msg:
                # æ”¶åˆ°æ–°ROSæ•°æ®ï¼Œæ›´æ–°å¹¶ä½¿ç”¨æœ€æ–°è§’åº¦
                angle_z_deg = tracking_data['angle_z_deg']
                last_valid_angle = angle_z_deg
                last_seen_ts = tracking_data['timestamp']
                print(f"ğŸ”„ ä½¿ç”¨ROSè·Ÿè¸ªè§’åº¦: {angle_z_deg:.2f}Â° ")
            else:
                # æ²¡æœ‰æ–°ROSæ•°æ®
                if last_valid_angle is not None:
                    angle_z_deg = last_valid_angle
                    print(f"ä½¿ç”¨ä¸Šæ¬¡ROSè§’åº¦: {angle_z_deg:.2f}Â° (å½“å‰æ— æ–°æ•°æ®)")
                else:
                    angle_z_deg = -45  # æœé‡Œ
                    print("ä»æœªæ¥æ”¶åˆ°ROSæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦: -45Â°")

            #mark: è·å–ROSå›¾åƒæ•°æ®å¹¶æ£€æµ‹æ–¹å‘ï¼ˆéé˜»å¡ï¼‰
            orientation_image, img_timestamp = ros_subscriber.get_latest_image()
            # has_new_image = orientation_image is not None and (
            #     last_seen_img_ts is None or img_timestamp > last_seen_img_ts
            # )
            has_new_image = orientation_image is not None
            
            if has_new_image:
                # æ”¶åˆ°æ–°å›¾åƒï¼Œè¿›è¡Œæ–¹å‘æ£€æµ‹
                print(f"ğŸ“· æ£€æµ‹æ–°å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
                detected_angles = detect_dent_orientation(orientation_image, save_dir=save_dir)
                
                if detected_angles:
                    # æ£€æµ‹æˆåŠŸï¼Œä¿å­˜ç»“æœ
                    last_valid_detected_angles = detected_angles
                    last_seen_img_ts = img_timestamp
                    print(f"è¾¹ç¼˜è®¡ç®—æ£€æµ‹åˆ°çš„ç‰©ä½“æœå‘è§’åº¦: {detected_angles}")
                else:
                    # æ£€æµ‹å¤±è´¥ï¼Œä½†æ›´æ–°æ—¶é—´æˆ³
                    last_seen_img_ts = img_timestamp
                    print("å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾")
                    # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨å†å²æ•°æ®
                    if last_valid_detected_angles is not None:
                        detected_angles = last_valid_detected_angles
                        print(f"å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾ï¼Œä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹åˆ°çš„è§’åº¦: {detected_angles}")
            else:
                # æ²¡æœ‰æ–°å›¾åƒ
                if last_valid_detected_angles is not None:
                    # ä½¿ç”¨ä¸Šæ¬¡æœ‰æ•ˆçš„æ£€æµ‹ç»“æœ
                    detected_angles = last_valid_detected_angles
                    print(f"å½“å‰æ— æ–°å›¾åƒï¼Œä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹åˆ°çš„è§’åº¦: {detected_angles}")
                else:
                    # ä»æœªæ£€æµ‹åˆ°è¿‡è§’åº¦ï¼ˆå¯èƒ½ä»æœªæ¥æ”¶åˆ°å›¾åƒï¼Œæˆ–æ¥æ”¶åˆ°ä½†ä»æœªæ£€æµ‹æˆåŠŸï¼‰
                    detected_angles = None
                    print("ä»æœªæ¥æ”¶åˆ°å›¾åƒï¼Œä¹Ÿå°±ä»æœªæ£€æµ‹åˆ°è§’åº¦")

            # è®°å½•angle_z_deg å’Œ detected_anglesåˆ°logæ–‡ä»¶
            with open(angle_log_path, 'a') as f:
                angles_str = str(detected_angles) if detected_angles is not None else "None"
                f.write(f"{frame_count},{time.time():.3f},{angle_z_deg:.2f},{angles_str}\n")
            
            # å°†center_poseè½¬æ¢ä¸ºnumpyæ•°ç»„
            center_pose_array = np.array(center_pose, dtype=float)
            
            # ------ä½¿ç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–------
            # é…ç½®æŠ“å–å‚æ•°
            z_xoy_angle = 0 # ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦
            vertical_euler = [-180, 0, -90]  # å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„rx, ry, rz
            grasp_tilt_angle = 30  #  ç”±å‚ç›´å‘ä¸‹æŠ“å–æ—‹è½¬ä¸ºæ–œç€å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„æ—‹è½¬è§’åº¦ï¼š åŠ äº†30åº¦ä¼šæœå¤–
            z_safe_distance= 32  #zæ–¹å‘çš„ä¸€ä¸ªå®‰å…¨è·ç¦»ï¼Œä¹Ÿæ˜¯ä¸ºäº†æŠ“å–ç‰©ä½“é ä¸Šçš„éƒ¨åˆ†ï¼Œå¯çµæ´»è°ƒæ•´
            
            # # è°ƒç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–
            # success, T_base_ee_ideal = execute_grasp_from_object_pose(
            #     center_pose_array=center_pose_array,
            #     dobot=dobot,
            #     gripper=gripper,
            #     T_ee_cam=T_ee_cam,
            #     z_xoy_angle=z_xoy_angle,
            #     vertical_euler=vertical_euler,
            #     grasp_tilt_angle=grasp_tilt_angle,
            #     angle_threshold=10.0,
            #     T_tcp_ee_z= -0.16, 
            #     T_safe_distance= 0.00, #å¯çµæ´»è°ƒæ•´
            #     z_safe_distance=z_safe_distance,
            #     gripper_close_pos=20,
            #     verbose=True
            # )


            # orientation_image, img_timestamp = ros_subscriber.get_latest_image()
            # if orientation_image is not None:
            #     print(f"ğŸ“· æ£€æµ‹å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
            #     detected_angles = detect_dent_orientation(orientation_image)
            #     if detected_angles:
            #         print(f"è¾¹ç¼˜è®¡ç®—æ£€æµ‹åˆ°çš„ç‰©ä½“æœå‘è§’åº¦: {detected_angles}")
            #     else:
            #         print("æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ–¹å‘ç‰¹å¾")
            # else:
            #     print("å°šæœªæ¥æ”¶åˆ°å›¾åƒæ•°æ®")
            

            #------è°ƒæ•´ç»ç’ƒæ£’å§¿æ€è‡³å‚ç›´å‘ä¸‹-------
            #!å½“å‚ç›´å‘ä¸‹ï¼Œangleä¸º-90åº¦æ—¶
            if angle_z_deg <= 0:
                target_angle_z_deg = -90
                delta_angle_z_deg = target_angle_z_deg - angle_z_deg #-90+45=-45
                delta_ee = -delta_angle_z_deg - grasp_tilt_angle
             #!å½“å‚ç›´å‘ä¸‹ï¼Œangleä¸º90åº¦æ—¶
            else:
                target_angle_z_deg = 90
                delta_angle_z_deg = target_angle_z_deg - angle_z_deg # 90 - 45 = 45
                delta_ee = delta_angle_z_deg - grasp_tilt_angle

            #éœ€è¦è®©tcpæœå¤–æ—‹è½¬
            
            pose_now = dobot.get_pose()
            pose_target = [pose_now[0], pose_now[1], pose_now[2], pose_now[3]+delta_ee, pose_now[4], pose_now[5] ]

            # dobot.move_to_pose(pose_target[0], pose_target[1], pose_target[2], pose_target[3], pose_target[4], pose_target[5], speed=7, acceleration=1) 


            # ç§»åŠ¨ç»ç’ƒæ£’åˆ°æŒ‡å®šä½ç½®

            success = True
            if success:
                print("\n[æˆåŠŸ] æŠ“å–æ“ä½œå®Œæˆ!")
                # input("æŒ‰Enterç»§ç»­...") #ä¸é€‚åˆåœ¨å¾ªç¯ä¸­ä½¿ç”¨
            else:
                print("\n[å¤±è´¥] æŠ“å–æ“ä½œæœªå®Œæˆ")
                
            # å¯é€‰ï¼šè¿”å›homeä½ç½®ï¼ˆæ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šï¼‰
            # dobot.move_to_pose(435.4503, 281.809, 348.9125, -179.789, -0.8424, 14.4524, speed=9)
    

    except KeyboardInterrupt:
        print("\n[ç”¨æˆ·ä¸­æ–­] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
    finally:
        cv2.destroyAllWindows()
        # dobot.disable_robot()

    #-------run demo---------

