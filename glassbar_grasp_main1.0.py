#!/usr/bin/env python3
import sys
sys.path.append("FoundationPose")
from estimater import *
from datareader import *
from dino_mask import get_mask_from_GD   
from create_camera import CreateRealsense
import cv2
import numpy as np
# import open3d as o3d
import pyrealsense2 as rs
# import torch
import time, os, sys
import json
import threading
# from ultralytics.models.sam import Predictor as SAMPredictor
from simple_api import SimpleApi, ForceMonitor, ErrorMonitor
from dobot_gripper import DobotGripper
from transforms3d.euler import euler2mat, mat2euler
from scipy.spatial.transform import Rotation as R
import queue
from spatialmath import SE3, SO3
from grasp_utils import normalize_angle, extract_euler_zyx, print_pose_info
from calculate_grasp_pose_from_object_pose import execute_grasp_from_object_pose
import rospy
from std_msgs.msg import Float64MultiArray
from sensor_msgs.msg import Image
from cv_bridge import CvBridge



# ---------- æ‰‹çœ¼æ ‡å®š ----------
def load_hand_eye_calibration(json_path="hand_eye_calibration.json"):
    """ä»JSONæ–‡ä»¶åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    calibration = data['T_ee_cam']
    rotation_matrix = np.array(calibration['rotation_matrix'])
    translation_vector = calibration['translation_vector']
    return SE3.Rt(rotation_matrix, translation_vector, check=False)

# ä»ç›¸æœºåæ ‡ç³»åˆ°æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»çš„å˜æ¢çŸ©é˜µ
T_ee_cam = load_hand_eye_calibration()

# ---------- æœºæ¢°è‡‚ ----------
def init_robot():
    dobot = SimpleApi("192.168.5.1", 29999)
    dobot.clear_error()
    dobot.enable_robot()
    dobot.stop()
    # å¯åŠ¨åŠ›ä¼ æ„Ÿå™¨
    dobot.enable_ft_sensor(1)
    time.sleep(1)
    # åŠ›ä¼ æ„Ÿå™¨ç½®é›¶(ä»¥å½“å‰å—åŠ›çŠ¶æ€ä¸ºåŸºå‡†)
    dobot.six_force_home()
    time.sleep(1)
    # åŠ›ç›‘æ§çº¿ç¨‹
    # force_monitor = ForceMonitor(dobot)
    # force_monitor.start_monitoring()
    # error_monitor = ErrorMonitor(dobot)
    # error_monitor.start_monitoring()
    gripper = DobotGripper(dobot)
    gripper.connect(init=True)
    return dobot, gripper


# ---------- ROSèŠ‚ç‚¹ ----------
class ROSSubscriberTest:
    def __init__(self):
        """åˆå§‹åŒ–ROSèŠ‚ç‚¹å’Œè®¢é˜…è€…"""
        rospy.init_node('ros_subscriber_test', anonymous=True)
        
        # åˆå§‹åŒ–cv_bridgeç”¨äºå›¾åƒè½¬æ¢
        self.bridge = CvBridge()
        
        #? çº¿ç¨‹å®‰å…¨çš„æ•°æ®å­˜å‚¨
        self.latest_tracking_data = {
            'angle_z_deg': 0.0,
            'b': 0.0,
            'x': 0.0,
            'y': 0.0,
            'timestamp': 0.0,
            'valid': False
        }
        self.data_lock = threading.Lock()
        
        # è®¢é˜…tracking_data topic
        self.tracking_sub = rospy.Subscriber(
            'tracking_data', 
            Float64MultiArray, 
            self.tracking_callback
        )
        
        # è®¢é˜…object_orientation topic  
        self.image_sub = rospy.Subscriber(
            'image_object_orientation',
            Image,
            self.image_callback
        )
        
        print("ROSè®¢é˜…è€…å·²å¯åŠ¨ï¼Œç­‰å¾…æ•°æ®...")
        
    def tracking_callback(self, msg):
        """å¤„ç†tracking_dataæ¶ˆæ¯"""
        if len(msg.data) >= 4:
            angle_z_deg = msg.data[0]
            b = msg.data[1] 
            x = msg.data[2]
            y = msg.data[3]
            
            #? çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°æ•°æ®
            with self.data_lock:
                self.latest_tracking_data.update({
                    'angle_z_deg': angle_z_deg,
                    'b': b,
                    'x': x,
                    'y': y,
                    'timestamp': time.time(),
                    'valid': True
                })
            
            # print(f"ğŸ“Š è·Ÿè¸ªæ•°æ®: è§’åº¦={angle_z_deg:.2f}Â°, æˆªè·={b:.6f}, ä½ç½®=({x:.6f}, {y:.6f})")
        else:
            pass
            # print(f"âš ï¸  è·Ÿè¸ªæ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›4ä¸ªå€¼ï¼Œå®é™…æ”¶åˆ°{len(msg.data)}ä¸ªå€¼")
    
    def image_callback(self, msg):
        """å¤„ç†object_orientationå›¾åƒæ¶ˆæ¯"""
        try:
            # å°†ROSå›¾åƒæ¶ˆæ¯è½¬æ¢ä¸ºOpenCVæ ¼å¼
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow("Object Orientation", cv_image)
            cv2.waitKey(1)  # éé˜»å¡ç­‰å¾…ï¼Œå…è®¸å…¶ä»–å¤„ç†
            
            # æ‰“å°å›¾åƒä¿¡æ¯
            height, width = cv_image.shape[:2]
            print(f"ğŸ–¼ï¸  æ¥æ”¶åˆ°å›¾åƒ: {width}x{height} pixels")
            
        except Exception as e:
            print(f"âŒ å›¾åƒå¤„ç†é”™è¯¯: {e}")
    
    #?
    def get_latest_tracking_data(self):
        """è·å–æœ€æ–°çš„è·Ÿè¸ªæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.data_lock:
            return self.latest_tracking_data.copy()
    
    def run(self):
        """è¿è¡Œè®¢é˜…è€…"""
        try:
            # éé˜»å¡ä¿æ´»å¾ªç¯ï¼šç­‰å¾…ROSäº‹ä»¶ï¼Œä½†ä¸ä¸»åŠ¨é€€å‡º
            while not rospy.is_shutdown():
                time.sleep(0.05)
        except KeyboardInterrupt:
            print("\n rosä¸­æ–­,æ­£åœ¨é€€å‡º...")


if __name__ == "__main__":
    camera = CreateRealsense("231522072272") #å·²åˆå§‹åŒ–ç›¸æœºã€‚ ç›¸æœºåˆ†è¾¨ç‡æ˜¯å¤šå°‘ï¼Ÿ
    mesh_file = "mesh/obj_C_07_02_G.obj"
    debug = 0
    debug_dir = "debug"
    set_logging_format()
    set_seed(0)
    mesh = trimesh.load(mesh_file)
    mesh.vertices /= 1000 #! å•ä½è½¬æ¢é™¤ä»¥1000
    #! ç»ç’ƒæ£’å°ºå¯¸æ›´å°ï¼Œä½†å¦‚æœmeshå·²ç»å¯¹äº†çš„è¯å°±ä¸ç”¨ç¼©å°ã€‚ç»ç’ƒæ£’å°ºå¯¸æ–‡ä»¶: obj_C_07_02_G.obj
    # mesh.vertices /= 3
    to_origin, extents = trimesh.bounds.oriented_bounds(mesh)
    bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)

    # åˆå§‹åŒ–æœºæ¢°è‡‚
    dobot, gripper = init_robot()

    #? åˆå§‹åŒ–ROSè®¢é˜…è€…ï¼ˆåœ¨åå°daemonçº¿ç¨‹è¿è¡Œï¼Œä¸ä¼šé˜»å¡mainç¨‹åºï¼‰
    try:
        ros_subscriber = ROSSubscriberTest()
        ros_thread = threading.Thread(target=ros_subscriber.run, daemon=True)
        ros_thread.start()
        print("âœ… ROSè®¢é˜…è€…å·²åœ¨åå°å¯åŠ¨ï¼ˆéé˜»å¡æ¨¡å¼ï¼‰")
        time.sleep(1)  # çŸ­æš‚ç­‰å¾…ROSèŠ‚ç‚¹å¯åŠ¨
    except Exception as e:
        print(f"âš ï¸  ROSè®¢é˜…è€…å¯åŠ¨å¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½å¯¹è±¡ï¼Œé˜²æ­¢åç»­ä»£ç å‡ºé”™
        class DummySubscriber:
            def get_latest_tracking_data(self):
                return {'valid': False, 'angle_z_deg': 0.0, 'b': 0.0, 'x': 0.0, 'y': 0.0, 'timestamp': 0.0}
        ros_subscriber = DummySubscriber()

    # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œå§¿æ€ä¼˜åŒ–å™¨
    scorer = ScorePredictor() 
    refiner = PoseRefinePredictor()
    glctx = dr.RasterizeCudaContext()
    # åˆ›å»ºFoundationPoseä¼°è®¡å™¨
    est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=debug, glctx=glctx)
    logging.info("estimator initialization done")
    # è·å–ç›¸æœºå†…å‚
    cam_k = np.loadtxt(f'cam_K.txt').reshape(3,3)

    
    try:
        frame_count = 0
        last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
        last_valid_angle = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„ROSè§’åº¦
        last_seen_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„ROSæ—¶é—´æˆ³
        
        while True:
            # è·å–å½“å‰å¸§
            # color = camera.get_frames()['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            # depth = camera.get_frames()['depth']/1000
            # ir1 = camera.get_frames()['ir1']
            # ir2 = camera.get_frames()['ir2']
            # è·å–å½“å‰å¸§ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼Œå¤ç”¨è¿”å›çš„æ‰€æœ‰é€šé“ï¼‰
            frames = camera.get_frames()
            if frames is None:
                continue
            color = frames['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            depth = frames['depth']/1000
            ir1 = frames['ir1']
            ir2 = frames['ir2']
            
            # cv2.imwrite("ir1.png", ir1)
            # cv2.imwrite("ir2.png", ir2)
            
            
            # æ¯éš”70å¸§è¿›è¡Œä¸€æ¬¡FoundationPoseæ£€æµ‹
            if frame_count % 70 == 0:
                #ä½¿ç”¨GroundingDINOè¿›è¡Œè¯­ä¹‰ç†è§£æ‰¾åˆ°ç‰©ä½“çš„ç²—ç•¥ä½ç½®ï¼ŒSAMè·å–ç‰©ä½“çš„ç›¸å¯¹ç²¾ç¡®æ©ç 
                mask = get_mask_from_GD(color, "stirring rod")
                # mask = get_mask_from_GD(color, "Plastic dropper")
            
                cv2.imshow("mask", mask)
                pose = est.register(K=cam_k, rgb=color, depth=depth, ob_mask=mask, iteration=50)
                print(f"ç¬¬{frame_count}å¸§æ£€æµ‹å®Œæˆï¼Œpose: {pose}")
                center_pose = pose@np.linalg.inv(to_origin) #! è¿™ä¸ªæ‰æ˜¯ç‰©ä½“ä¸­å¿ƒç‚¹çš„Pose
                vis = draw_posed_3d_box(cam_k, img=color, ob_in_cam=center_pose, bbox=bbox)
                vis = draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=cam_k, thickness=3, transparency=0, is_input_rgb=True)
                cv2.imshow('1', vis[...,::-1])
                # cv2.waitKey(0) #waitKey(0) æ˜¯ä¸€ç§é˜»å¡
                # input("break001") #inputä¹Ÿæ˜¯ä¸€ç§é˜»å¡
                # print("break001")
                last_valid_pose = center_pose  # ä¿å­˜è¿™æ¬¡æ£€æµ‹çš„ç»“æœ
            else:
                # ä½¿ç”¨ä¸Šä¸€æ¬¡æ£€æµ‹çš„ç»“æœ
                center_pose = last_valid_pose
                print(f"ç¬¬{frame_count}å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœ")
            

            print("center_pose_object: ", center_pose) 
            frame_count += 1

            key = cv2.waitKey(1)
            # if key == ord('q'):  # æŒ‰qé€€å‡º
            #     break
            # elif key == ord('a'):  # æŒ‰aæ‰§è¡ŒæŠ“å–

            #? è·å–ROSè·Ÿè¸ªæ•°æ®ï¼ˆéé˜»å¡ï¼Œä¸ä¼šåœæ­¢mainç¨‹åºï¼‰
            tracking_data = ros_subscriber.get_latest_tracking_data()
            
            has_new_msg = tracking_data['valid'] and (
                last_seen_ts is None or tracking_data['timestamp'] > last_seen_ts
            )
            
            if has_new_msg:
                # æ”¶åˆ°æ–°ROSæ•°æ®ï¼Œæ›´æ–°å¹¶ä½¿ç”¨æœ€æ–°è§’åº¦
                angle_z_deg = tracking_data['angle_z_deg']
                last_valid_angle = angle_z_deg
                last_seen_ts = tracking_data['timestamp']
                print(f"ğŸ”„ ä½¿ç”¨ROSè·Ÿè¸ªè§’åº¦: {angle_z_deg:.2f}Â° ")
            else:
                # æ²¡æœ‰æ–°ROSæ•°æ®
                if last_valid_angle is not None:
                    angle_z_deg = last_valid_angle
                    print(f"ä½¿ç”¨ä¸Šæ¬¡ROSè§’åº¦: {angle_z_deg:.2f}Â° (å½“å‰æ— æ–°æ•°æ®)")
                else:
                    angle_z_deg = -45  # æœé‡Œ
                    print("ä»æœªæ¥æ”¶åˆ°ROSæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦: -45Â°")

            
            # å°†center_poseè½¬æ¢ä¸ºnumpyæ•°ç»„
            center_pose_array = np.array(center_pose, dtype=float)
            
            # ------ä½¿ç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–------
            # é…ç½®æŠ“å–å‚æ•°
            z_xoy_angle = 0 # ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦
            vertical_euler = [-180, 0, -90]  # å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„rx, ry, rz
            grasp_tilt_angle = 30  #  ç”±å‚ç›´å‘ä¸‹æŠ“å–æ—‹è½¬ä¸ºæ–œç€å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„æ—‹è½¬è§’åº¦ï¼š åŠ äº†30åº¦ä¼šæœå¤–
            z_safe_distance= 15  #zæ–¹å‘çš„ä¸€ä¸ªå®‰å…¨è·ç¦»ï¼Œä¹Ÿæ˜¯ä¸ºäº†æŠ“å–ç‰©ä½“é ä¸Šçš„éƒ¨åˆ†ï¼Œå¯çµæ´»è°ƒæ•´
            
            # è°ƒç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–
            # success, T_base_ee_ideal = execute_grasp_from_object_pose(
            #     center_pose_array=center_pose_array,
            #     dobot=dobot,
            #     gripper=gripper,
            #     T_ee_cam=T_ee_cam,
            #     z_xoy_angle=z_xoy_angle,
            #     vertical_euler=vertical_euler,
            #     grasp_tilt_angle=grasp_tilt_angle,
            #     angle_threshold=10.0,
            #     T_tcp_ee_z= -0.16, 
            #     T_safe_distance=0.003, #å¯çµæ´»è°ƒæ•´
            #     z_safe_distance=z_safe_distance,
            #     verbose=True
            # )
            

            #è°ƒæ•´ç»ç’ƒæ£’å§¿æ€è‡³å‚ç›´å‘ä¸‹: 
            #å½“å‚ç›´å‘ä¸‹ï¼Œangleä¸º-90åº¦æ—¶
            angle_z_deg = -45 #æœé‡Œ
            target_angle_z_deg = -90
            delta_angle_z_deg = target_angle_z_deg - angle_z_deg #-90+45=-45
            #éœ€è¦è®©tcpæœå¤–æ—‹è½¬
            delta_ee = -delta_angle_z_deg - grasp_tilt_angle

            #!å½“å‚ç›´å‘ä¸‹ï¼Œangleä¸º90åº¦æ—¶
            

            pose_now = dobot.get_pose()
            pose_target = [pose_now[0], pose_now[1], pose_now[2], pose_now[3]+delta_ee, pose_now[4], pose_now[5] ]

            # dobot.move_to_pose(pose_target[0], pose_target[1], pose_target[2], pose_target[3], pose_target[4], pose_target[5], speed=7, acceleration=1) 


            # ç§»åŠ¨ç»ç’ƒæ£’åˆ°æŒ‡å®šä½ç½®

            success = True
            if success:
                print("\n[æˆåŠŸ] æŠ“å–æ“ä½œå®Œæˆ!")
                # input("æŒ‰Enterç»§ç»­...") #ä¸é€‚åˆåœ¨å¾ªç¯ä¸­ä½¿ç”¨
            else:
                print("\n[å¤±è´¥] æŠ“å–æ“ä½œæœªå®Œæˆ")
                
            # å¯é€‰ï¼šè¿”å›homeä½ç½®ï¼ˆæ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šï¼‰
            # dobot.move_to_pose(435.4503, 281.809, 348.9125, -179.789, -0.8424, 14.4524, speed=9)
    

    except KeyboardInterrupt:
        print("\n[ç”¨æˆ·ä¸­æ–­] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
    finally:
        cv2.destroyAllWindows()
        # dobot.disable_robot()

    #-------run demo---------

